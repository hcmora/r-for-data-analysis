#####################################
### Best Subsect Selection
#####################################

library(ISLR)
fix(Hitters)
names(Hitters)
dim(Hitters)
sum(is.na(Hitters))
# We check that there are 59 salaries missing.
# Due to the fact that this is what will be predicted, we'll omit these

Hitters = na.omit(Hitters)
sum(is.na(Hitters))

# The regsubsets() function performs best subset selection by identifying 
# the best model that contains a given number of predictors

# install.packages('leaps')
library(leaps)
?regsubsets
regfit.full = regsubsets(x=Salary~.,data=Hitters)
summary(regfit.full)

# By default, regsubsets() reports the results up to the best eight-variable
# model. To change this, we add the nvmax attribute

regfit.full = regsubsets(x=Salary~.,data=Hitters,nvmax = 19)
reg.summary = summary(regfit.full)

names(reg.summary)

# If we check the R^2 statistic, we can observe that it increases monotonically
# as more variables are included

reg.summary$rsq

# We'll now plot the RSS, adjusted R^2, Cp and BIC for all the models to decide
# which one is the best

par(mfrow=c(2,2))
plot(reg.summary$rss,xlab = "Number of Variables", ylab = "RSS", type = "l")
plot(reg.summary$adjr2,xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")

# We'll add a point to the adjusted R^2 plot where the max occurs
max_point = which.max(reg.summary$adjr2)
points(max_point, reg.summary$adjr2[max_point], col="red", cex=2, pch=20)

# We'll do the same for Cp and BIC statistics, but now we search for the minimum
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp", type="l")
min_point = which.min(reg.summary$cp)
points(min_point, reg.summary$cp[min_point], col="red", cex=2, pch=20)

plot(reg.summary$bic, xlab="Number of Variables", ylab="BIC", type="l")
min_point = which.min(reg.summary$bic)
points(min_point, reg.summary$bic[min_point], col="red", cex=2, pch=20)

?plot.regsubsets

# The built in plot function in regsubsets shows the selected variables for 
# the best model with a given number of predictors ranking accordingly to 
# the parameter introduced

plot(regfit.full,scale="r2")
plot(regfit.full,scale="adjr2")
plot(regfit.full,scale="Cp")
plot(regfit.full,scale="bic")

# Expanding the Files-Plot-Packages-Help-Viewer window may allow to avoid an error
# Depending on which parameter weÂ´re using to determine the best model, we'll choose
# different variables

# For example, the lowest BIC model is with 6 variables, we can check it with coef()
coef(regfit.full,6)

#####################################
### Forward and Backward Stepwise 
### Selection
#####################################

# regsubsets() can also perform forward and backward stepwise selection
# by specifying the argument method

regfit.fwd = regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="r2")
plot(regfit.fwd,scale="adjr2")
plot(regfit.fwd,scale="Cp")
plot(regfit.fwd,scale="bic")

regfit.bwd = regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
summary(regfit.bwd)

#####################################
### Choosing Among Models Using the
### Validation Set Approach and
### Cross-Validation
#####################################

# We create our training and test data
set.seed(1)
train = sample(c(TRUE,FALSE), nrow(Hitters),rep = TRUE)
test = (!train)
regfit.best = regsubsets(Salary~.,data=Hitters[train,],nvmax=19)

# To test each of the models, we first need to create an X matrix
# with the test data that was set apart

test.mat = model.matrix(Salary~.,data=Hitters[test,])

# Now for each model generated by regsubsets, we extract their coefficients
# multiply them with our new test matrix to obtain our predicted Y values
# and then we compute the test MSE

val.errors = rep(NA,19)
for (i in 1:19){
  coefi = coef(regfit.best,id=i)
  pred = test.mat[,names(coefi)]%*%coefi
  val.errors[i] = mean((Hitters$Salary[test]-pred)^2)
}
par(mfrow=c(1,1))
plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,500),pch=19,type="b")

# We can add the RSS points to check the difference between the test error and RSS
points(sqrt(regfit.best$rss[-1]/75),col="blue",pch=19,type="b")
legend("topright",legend=c("Training","Validation"),col=c("blue","black"),pch=19)

# Some explanation notes:
# on the first line of the for loop, we obtain a vector with the coefficients
# of each model, going from 1 to 19
# on the second line, we make a matrix multiplication (that's what %*% do) of
# the columns of the test matrix that are related to the coefficients selected
# by our model with their correspondent coefficient. This creates a vector (or a
# mx1 matrix) with the predicted value for each row of our test matrix
# On the last line, we compute the MSE by calculating the squared difference between
# the actual value and the predicted value.

which.min(val.errors)
# In this case, the best model is with 10 variables
coef(regfit.best,which.min(val.errors))

# Since there is no predict function asociated to regsubsets, we'll create one.
# The as.formula is used to extract the Salary~. used to create the regsubsets
# This function will be useful for the cross-validation process

predict.regsubsets = function(object,newdata,id,...) {
  form = as.formula(object$call[[2]])
  mat = model.matrix(form, newdata)
  coefi = coef(object,id = id)
  xvars = names(coefi)
  mat[,xvars]%*%coefi
}

# Now that we know that 10 variables is the best model available, we can
# train our model with the full data

regfit.best = regsubsets(Salary~.,data=Hitters,nvmax=19)
coef(regfit.best,id=10)

### Cross-Validation

# We first create a vector that allocates each row to a k-fold
k = 10
set.seed(1)
folds = sample(1:k,nrow(Hitters),replace=TRUE)

# Empty matrix creation, with the size of k x variables
cv.errors = matrix(NA,k,19, dimnames = list(NULL, paste(1:19)))

# Now we create a for loop where j will represent the test set selected and
# i the quantity of variables that will be used in the model
# Each j,i element represent the MSE of the model generated for the j-fold
# with the ith dimension model

for (j in 1:k){
  best.fit = regsubsets(Salary~.,data=Hitters[folds != j,],nvmax=19)
  for (i in 1:19){
    pred = predict.regsubsets(best.fit,newdata = Hitters[folds == j,],id=i)
    cv.errors[j,i] = mean((Hitters$Salary[folds==j]-pred)^2)
  }
}

# Now we compute the mean of the MSE for each model dimension

mean.cv.errors = apply(cv.errors,2,mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors,type="b")

# From the plot we can see that cross-validation selects an 11-variable model

reg.best = regsubsets(Salary~.,data=Hitters,nvmax=12)
coef(reg.best,id=11)
